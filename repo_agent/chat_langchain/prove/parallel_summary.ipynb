{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your own MAP Reduce where map is parallel and reduce is sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(path):\n",
    "        all_docs = []\n",
    "        abs = os.path.normpath(os.path.abspath(path))  # Normalize and convert root_path to an absolute path\n",
    "\n",
    "        for subdir, _, _ in os.walk(abs):\n",
    "            loader = DirectoryLoader(os.path.join(abs, subdir), glob=\"./*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)    \n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "        return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(doc, chunk_size=250, chunk_overlap=30):\n",
    "        \"\"\" Split a document into chunks of text.\"\"\"\n",
    "\n",
    "        headers_to_split_on = [\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "        )\n",
    "        md_header_splits = markdown_splitter.split_text( doc.page_content)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        # Split\n",
    "        splits = text_splitter.split_documents(md_header_splits)\n",
    "    \n",
    "        return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:24<00:00, 12.44s/it]\n",
      "100%|██████████| 13/13 [00:07<00:00,  1.67it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.69it/s]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.90it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.07it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.47it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = load_docs(\"C:\\\\Users\\\\reply\\\\RepoAgent\\\\markdown_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = []\n",
    "for doc in docs:\n",
    "        splits = split_documents(doc, chunk_size=5000, chunk_overlap=0)\n",
    "        for doc_split in splits:\n",
    "            filename = os.path.basename(list(doc.metadata.values())[0])\n",
    "            doc_split.metadata = {'source':filename}        \n",
    "        all_splits.extend(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(stuff_chain, docs):\n",
    "    summary = []\n",
    "    for doc in docs:\n",
    "        summary.extend(stuff_chain.invoke([doc])[\"output_text\"])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s 1500 -> 8.38 min\n",
    "s 5000 -> 3.59 min\n",
    "mr -> 2.26 min\n",
    "s 5000 + parallel -> 18 sec \n",
    "\n",
    "reduce: \n",
    "4sec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main function copies Markdown documentation files from a specified folder to a destination folder, creates a README.md file if it does not exist, and organizes the copied files accordingly. It checks if the destination directory exists and creates it if necessary. It then iterates through the items in the source directory and copies them to the destination directory. After copying all the files, it checks if a README.md file exists in the destination directory and creates one if necessary. The create_book_readme_if_not_exist function creates a README.md file in the specified directory if it does not already exist.\n",
      "The given code includes four functions: create_readme_if_not_exist, output_markdown, markdown_file_in_dir, and is_markdown_file. \n",
      "\n",
      "The create_readme_if_not_exist function creates a README.md file in a specified directory if it does not already exist. \n",
      "\n",
      "The output_markdown function generates a markdown summary of files and directories within a specified directory, including creating markdown links to README.md files and markdown files. \n",
      "\n",
      "The markdown_file_in_dir function checks if there are any Markdown (.md) or Markdown (.markdown) files in a specified directory. \n",
      "\n",
      "The is_markdown_file function checks if a given filename corresponds to a Markdown file based on its extension. \n",
      "\n",
      "The main function is used to create a markdown summary file for a specified book directory, including markdown links to relevant files and directories.\n",
      "The main function creates a book directory and a SUMMARY.md file within it. It then generates a markdown summary based on the contents of the book directory. Finally, it prints a completion message. The function relies on the output_markdown function for markdown generation.\n",
      "The ClassDef ChangeDetector is responsible for handling file differences and change detection in a Git repository. It has attributes for the repository path and the Git repository object. The class provides methods to track and analyze changes in files, such as retrieving staged Python files, getting file differences, parsing differences, identifying changes in structure, and retrieving unstaged files. The class also has an initialization function and a function to retrieve staged Python files. However, there may be some issues with the identify_changes_in_structure and get_to_be_staged_files methods that require further testing and improvement.\n",
      "The GitPython library has different logic than standard Git behavior, particularly when handling new files in the staging area. The use of the R=True parameter is important for correctly comparing and identifying newly added files. \n",
      "\n",
      "The get_file_diff function retrieves the changes made to a specific file, using git diff --staged for new files and getting differences from the HEAD for existing files. It returns a list of changes made to the file. \n",
      "\n",
      "The parse_diffs function processes a list of differences, extracting added and deleted line information and returning a dictionary with this information. It is used to identify changes in structure within Python files and manage version control operations. \n",
      "\n",
      "The identify_changes_in_structure function identifies the structures (functions or classes) where changes have occurred based on the provided changed lines and structures list. It is used to update project hierarchy information and create Markdown files based on the identified changes.\n",
      "The given information describes two functions: get_to_be_staged_files and add_unstaged_files. \n",
      "\n",
      "The get_to_be_staged_files function retrieves all unstaged files in a repository that meet specific conditions and returns a list of their paths. It identifies unstaged files based on two conditions: files whose extension changes to .md correspond to staged files, and files whose path matches the 'project_hierarchy' field in the configuration. The function handles both untracked and unstaged files and interacts with the Git repository to accurately identify staged and unstaged files.\n",
      "\n",
      "The add_unstaged_files function adds unstaged files that meet specific conditions to the staging area in a Git repository. It calls the get_to_be_staged_files method to retrieve the list of unstaged files, constructs Git commands to add each file to the staging area, and uses the subprocess.run function to execute the Git commands. The function returns the list of unstaged files that were added to the staging area.\n",
      "The \"get_import_statements\" function extracts import statements from the source code of the current module. It uses the inspect module to retrieve the source code lines and filters out lines starting with \"import\" or \"from\". The function returns a list of import statements found in the source code. This function is useful for analyzing module dependencies. \n",
      "\n",
      "The \"ResponseMessage\" class stores a string content representing a response message. It has one attribute, \"content\", which stores the actual text content of the response message. This class is used in the \"attempt_generate_response\" method of the \"ChatEngine\" class to handle different scenarios during response generation. It serves as a container for response messages within the project.\n",
      "\n",
      "The \"ChatEngine\" class generates documentation for functions or classes. It has several methods, including \"num_tokens_from_string\" to count the number of tokens in a string, \"reduce_input_length\" to shorten input prompts, \"generate_response\" to generate a response message using the OpenAI API, \"attempt_generate_response\" to attempt generating a response message with retries, and \"generate_doc\" to generate documentation for a given item.\n",
      "The ChatEngine class is a crucial part of the documentation generation process. It has methods for calculating the number of tokens in a string, reducing input prompt length, generating a response using the OpenAI API, and handling errors. It works together with other classes and functions to create comprehensive and accurate documentation. The init function initializes the ChatEngine object with a project manager. The num_tokens_from_string function returns the number of tokens in a text string. The reduce_input_length function shortens input prompts. The generate_response function interacts with the OpenAI API to generate a response based on the provided parameters.\n",
      "The \"attempt_generate_response\" function attempts to generate a response by calling the \"generate_response\" function multiple times in case of errors. It handles API connection errors and other exceptions by logging the errors, waiting for a specified time, and retrying the request. If the maximum number of attempts is reached, it either raises an exception or returns a predefined response message. The function is part of the ChatEngine class and takes parameters such as the model, system prompt, user prompt, maximum tokens, and maximum attempts. The \"generate_doc\" function generates documentation for a given object by retrieving information from a DocItem object and a file_handler object. It also checks if the code is referenced by other objects or if it references other objects.\n",
      "The given text describes a function called \"generate_doc\" that is used to automatically generate documentation for code objects in a project. The function uses a ProjectManager instance to build the hierarchical structure of the project and retrieves information about objects that reference the code and objects that are referenced by the code. It prepares prompts for the OpenAI chat model by combining relevant information and handles cases where the prompt exceeds the model's limit. The function sends a request to the chat model to generate the documentation and handles potential errors. The generated documentation is returned as a response message, including information about referencing and referenced objects. The function also provides an output example of the code's return value. The function relies on the ChatEngine and ResponseMessage classes. The text also describes another function called \"get_referenced_prompt\" that generates a prompt detailing the objects referenced by a given DocItem. It constructs prompts for each referenced object and returns them as a single string.\n",
      "The first function, get_referencer_prompt, generates a prompt that lists the objects referencing a specific DocItem object. It includes the object's name, documentation, and code snippet in the prompt. The function returns an empty string if there are no referencing objects.\n",
      "\n",
      "The second function, get_relationship_description, provides a description of the relationship between referencer content and reference letter in a project. It returns a description including the relationship with both callers and callees if both referencer_content and reference_letter are present. It returns a description including the relationship with callers if only referencer_content is present, and it returns a description including the relationship with callees if only reference_letter is present. If neither referencer_content nor reference_letter is present, it returns an empty string.\n",
      "The given code description describes three functions: get_config_path, read_config, and write_config. \n",
      "\n",
      "1. get_config_path: This function retrieves the path to the configuration file used by the application. It checks the current working directory for a configuration file named config.toml. If not found, it determines the appropriate configuration path based on the operating system. It ensures the configuration directory exists and creates an empty configuration file if necessary. It is called by other functions in the config_manager.py module.\n",
      "\n",
      "2. read_config: This function reads a configuration file specified by the file_path parameter or determines the path using get_config_path if no file_path is provided. It returns the contents of the configuration file as a dictionary. It handles decoding errors and is typically called by other parts of the application that require access to configuration settings.\n",
      "\n",
      "3. write_config: This function updates the existing configuration with new key-value pairs and writes the updated configuration back to a file. It takes a dictionary containing the new key-value pairs and an optional file_path parameter. If no file_path is provided, it determines the file path internally using get_config_path. It handles cases where the file might not exist or is empty. It is called by other parts of the project to save settings to the configuration file.\n",
      "The function simplifies the process of updating a configuration file by handling file operations internally and managing any errors that may occur. It emphasizes the importance of using a valid dictionary for the update_config parameter.\n",
      "The EdgeType class defines different types of edges in a graph, such as reference edges, subfile edges, and file item edges. It is used to categorize and differentiate between various types of edges in a graph. \n",
      "\n",
      "The DocItemType class defines the possible types of object documentation in a project, such as root, directory, file, class, function, sub-function, and global variable. It provides methods to convert the DocItemType to a string representation and print it with colored formatting. The class is used to categorize and identify different types of objects in the project hierarchy. The get_edge_type() method is currently empty and not implemented.\n",
      "The given information describes three different functions and a class in a codebase. \n",
      "\n",
      "The first function, \"print_self\", determines the color based on the type of a document item and returns a formatted string including the name of the item. It is used within the \"print_recursive\" function to print the type and name of an item.\n",
      "\n",
      "The second function, \"get_edge_type\", retrieves the type of edge that connects two specified item types. It is used to analyze relationships between different types of items within a document.\n",
      "\n",
      "The third class, \"DocItemStatus\", represents the status of a documentation item. It has predefined attributes that indicate the state of the item, such as whether the documentation is up to date or needs to be generated. This class is used to determine the status of a documentation item and make decisions based on it, such as whether to generate or update the documentation.\n",
      "The first code snippet describes the DocItemStatus class, which is used to represent the status of a documentation item and determine if it needs to be generated or updated. The need_to_generate function is also explained, which checks the status of a documentation item and other conditions to determine if it needs to be generated. It also has an optional ignore_list parameter to specify file paths to be ignored during generation.\n",
      "\n",
      "The second code snippet describes the has_ans_relation function, which checks if there is an ancestor relationship between two nodes in a tree structure and returns the earlier node if it exists. It is used in the walk_file function of the MetaInfo class to determine ancestor relationships between nodes representing objects in a codebase.\n",
      "The example demonstrates the usage of the has_ans_relation function by creating two DocItem objects, node_a and node_b, and assuming that node_a is an ancestor of node_b.\n",
      "The code provided includes four functions: get_travel_list, check_depth, parse_tree_path, and get_file_name. \n",
      "\n",
      "- The get_travel_list function traverses a tree structure in a pre-order manner and returns a list of nodes.\n",
      "- The check_depth function recursively calculates the depth of a node in a tree structure.\n",
      "- The parse_tree_path function recursively parses the tree path by appending the current node to the given path.\n",
      "- The get_file_name function retrieves the file name of an object by manipulating the full name of the object.\n",
      "The first function, get_full_name, retrieves the full name of an object by traversing through its parent objects. It has an optional parameter to include names of objects with duplicates. The second function, find, searches for a specific file in the repository hierarchy based on a given list of file paths. It checks if the current object is the root node and iterates through the file paths to find the desired file. The third function, check_has_task, recursively checks if a documentation item or its children require task generation based on certain conditions. It has an optional parameter to ignore specific file paths during the task generation process.\n",
      "The code description explains two functions: check_has_task and print_recursive. \n",
      "\n",
      "The check_has_task function determines if a documentation item or its children require task generation. It sets the has_task attribute of the current item to True if task generation is needed. It recursively checks through the hierarchy of documentation items to update the has_task attribute accordingly.\n",
      "\n",
      "The print_recursive function prints the repository objects in a hierarchical manner. It takes optional parameters to control the printing behavior, such as the level of indentation, whether to print the content of the objects, whether to print the difference status of the objects, and a list of file paths to be ignored during printing. It uses the print_indent helper function to generate the indentation string and determines the name to be printed for each object based on its item_type attribute. It recursively calls itself on the children of each object to print the hierarchy. The function is primarily used in the print_hierarchy and diff functions in the main.py file.\n",
      "The given information describes three different components: a function called print_indent, a function called find_all_referencer, and a class called MetaInfo. \n",
      "\n",
      "The print_indent function generates an indented string with a specified number of spaces. It takes an integer parameter called indent and returns an empty string if the indent value is 0. Otherwise, it generates an indented string by concatenating two spaces multiplied by the indent value, followed by \"|-\". It is important to ensure that the indent parameter is a non-negative integer.\n",
      "\n",
      "The find_all_referencer function locates all references to a specific variable in a given script file. It takes several parameters including the repository path, variable name, file path, line number, column number, and a boolean flag to indicate whether to search for references only within the same file. It utilizes the Jedi library to analyze the script file and searches for references to the variable at the provided location. It then filters out the references that match the variable name and returns a list of tuples containing the referencing file's relative path, line number, and column number. If an error occurs, it logs the error message and returns an empty list.\n",
      "\n",
      "The MetaInfo class manages metadata information related to the documentation generation process. It has various attributes and methods to handle initialization, loading, and saving of metadata, parsing reference relationships, and generating task lists for document generation. The class contains attributes such as the repository path, document version, target repository hierarchical tree, white list, fake file reflection mapping, jump files, deleted items from older metadata, and flags indicating the generation process and checkpoint operation. The class also has methods to initialize the MetaInfo object, load from a checkpoint directory, save to a directory, and print task lists.\n",
      "The MetaInfo class is responsible for managing metadata related to documentation generation. It provides methods for initializing metadata, loading and saving metadata, printing task lists, finding objects in the repository, parsing reference relationships, generating task managers, calculating object order, merging documentation from older versions, and converting metadata to JSON. The init_meta_info function initializes the MetaInfo object by generating the repository structure.\n",
      "The init_meta_info function is used to initialize the MetaInfo object and create the hierarchical representation of the target repository. It relies on the FileHandler class and takes parameters such as file_path_reflections and jump_files. The function returns a MetaInfo object that contains information about the repository path, fake file reflections, and jump files.\n",
      "\n",
      "The from_checkpoint_path function reads meta-information from an existing checkpoint directory and populates a MetaInfo object with the retrieved data. It reads project_hierarchy.json and meta-info.json files, extracts relevant meta-data, and assigns it to the corresponding attributes of the MetaInfo object. The function also prints a message indicating the loading of MetaInfo from the checkpoint directory.\n",
      "\n",
      "The checkpoint function saves the MetaInfo object to a specified directory. It acquires a lock for thread safety, creates the target directory if it doesn't exist, and calls the to_hierarchy_json function to obtain a hierarchical JSON representation of the document metadata. The function writes the JSON representation to a file named \"project_hierarchy.json\" and other metadata to a file named \"meta-info.json\" in the target directory.\n",
      "\n",
      "Developers can use the checkpoint function to save the MetaInfo object and its associated metadata for easy retrieval and storage. The saved files include \"project_hierarchy.json\" and \"meta-info.json\". The flash_reference_relation parameter can be used to include bidirectional reference relations in the saved MetaInfo.\n",
      "The provided information describes four different functions and their purposes in a project. \n",
      "\n",
      "1. The \"print_task_list\" function is used to display a table of task information, including task ID, generation reason, path, and dependencies. It uses the PrettyTable library to create the table and is called within the Runner class to print the task list before processing tasks for document generation.\n",
      "\n",
      "2. The \"get_all_files\" function retrieves all file nodes from a hierarchical tree structure. It recursively traverses the tree and appends file nodes to a list called \"files\". The function assumes the tree structure has a \"children\" attribute that is a dictionary of child nodes.\n",
      "\n",
      "3. The \"walk_tree\" function recursively traverses a tree structure starting from a given node and collects all leaf nodes of type \"_file\". It uses a depth-first search approach to ensure all leaf nodes are visited and added to a list called \"files\".\n",
      "\n",
      "4. The \"find_obj_with_lineno\" function finds the DocItem object that corresponds to a specific line number within a file. It takes a file node and a line number as input and searches for the corresponding object.\n",
      "The find_obj_with_lineno function searches for a specific line number within a file by iterating through its children. If a child is found that corresponds to the line number, the function updates the current node and continues the search. If no child is found, the function returns the current node. The parse_reference function extracts bidirectional reference relationships for all objects in a repository by iterating through file nodes and performing various checks and operations.\n",
      "The given code description explains two functions: \"walk_file\" and \"get_task_manager\". \n",
      "\n",
      "The \"walk_file\" function is a recursive function that traverses all variables within a file and finds their references. It uses a whitelist to filter objects and checks for various conditions before processing references. It also updates the reference relationships between objects and prints messages for skipped references and errors.\n",
      "\n",
      "The \"get_task_manager\" function generates a TaskManager object based on the topology of objects in the repository. It takes the current instance of the MetaInfo class, a starting node, and a function to determine if a node is available for task generation as parameters.\n",
      "The get_task_manager function generates a TaskManager object that manages tasks based on the topology of objects in the repository. It filters and sorts a list of DocItem nodes, calculates break levels for each node, and adds tasks to the task_manager based on dependencies. The in_white_list function is used to filter DocItem nodes based on predefined criteria. The function handles circular references and unresolved dependencies. The task_available_func function determines if a DocItem node is available for task generation. The function returns the task_manager once all DocItem nodes have been processed. The in_white_list function checks if an item is in the white list based on its file name and object name. It returns True if a match is found and False otherwise.\n",
      "The \"get_topology\" function calculates the topological order of objects in a repository by extracting reference relationships and generating a TaskManager object. It handles circular references and unresolved dependencies. The \"get_topology\" function takes in the current instance of the MetaInfo class and a task_available_func function as parameters. The \"_map\" function applies a specified operation to all nodes in a hierarchical tree structure, while the \"travel\" function recursively traverses through the children of a given DocItem object.\n",
      "The travel function is used to traverse a tree structure represented by DocItem objects, visiting each node and its children. It relies on the deal_func function to process each object during traversal. The load_doc_from_older_meta function merges documentation from an older version of the meta info into the current version. It uses nested functions to find and update corresponding items in the new version based on the older version. It also updates reference relationships in the new version. The function logs an informational message and stores deleted items from the older version.\n",
      "The load_doc_from_older_meta function assumes a valid hierarchical tree structure in the target repository. It relies on the find_item function to locate corresponding items in the new version of the meta info. It uses the travel function to update markdown content and status, and the travel2 function to update reference relationships in the new version. The function also calls the parse_reference method to extract bidirectional reference relationships. It updates the deleted_items_from_older_meta attribute with deleted items from the older version. The find_item function searches for an item in the new version based on its original item, ensuring consistency and accuracy in the meta information.\n",
      "The \"travel\" function is a recursive function that searches for an item in a new version of metadata based on its original item. It updates the metadata of the result item with the metadata of the original item if found, and adds the name and type of the original item to a list of deleted items if not found. The \"travel2\" function recursively traverses the hierarchy of DocItem objects and updates their item_status based on changes in their references. It compares the references of the result_item with the references of the now_older_item and updates the item_status accordingly. Both functions are important for updating metadata and tracking the status of documentation items.\n",
      "The item_status of DocItem objects has been updated due to changes in their references.\n",
      "The given code includes four functions: from_project_hierarchy_path, to_hierarchy_json, walk_file, and from_project_hierarchy_json. \n",
      "\n",
      "The from_project_hierarchy_path function parses a JSON representation of a project hierarchy, extracts information from the specified repository path, and converts it into a structured MetaInfo object. It checks the existence of the project_hierarchy.json file, reads its content, and uses the from_project_hierarchy_json function to convert it into a hierarchical structure represented by a MetaInfo object.\n",
      "\n",
      "The to_hierarchy_json function converts document metadata to a hierarchical JSON representation. It retrieves information about each object in the metadata and constructs a JSON structure with relevant metadata, including references to and from other objects. It provides a structured overview of the document's content and relationships between objects.\n",
      "\n",
      "The walk_file function recursively traverses a hierarchy of DocItem objects and updates the content of each object in a JSON-like format. It calls itself on each child of the current DocItem object.\n",
      "\n",
      "The from_project_hierarchy_json function parses a JSON representation of a project hierarchy and constructs a MetaInfo object that represents the hierarchical structure of the project.\n",
      "The \"from_project_hierarchy_json\" function takes in a dictionary representing the project hierarchy in JSON format. It creates a hierarchical tree structure based on the dictionary, checking if files exist and adding them as nodes in the tree. It also processes the content of the files and determines parent-child relationships between items. The function returns a MetaInfo object representing the hierarchical structure of the project. \n",
      "\n",
      "The \"change_items\" function recursively updates the item_type attribute of a DocItem based on its content and relationship with its parent item. It categorizes items as classes, functions, class functions, or sub-functions.\n",
      "\n",
      "The \"code_contain\" function determines if one code item contains another code item within its start and end lines.\n",
      "The code_contain function checks if one code item is contained within another code item by comparing their start and end lines. If the end line of the second item is less than the end line of the first item or the start line of the second item is greater than the start line of the first item, the function returns False. Otherwise, it returns True. The function does not consider the case where the start and end lines of the two items are equal.\n",
      "The ErrorHandler class is used to handle different types of exceptions and log appropriate messages based on the type of exception. It contains a static method called handle_exception that checks the type of the exception and logs a specific message. The OpenAIError class is a custom exception class for OpenAI related errors and is used in the ErrorHandler class to handle exceptions. The init function is a constructor method for the OpenAIError class that initializes the error message. Developers can use these classes and functions to handle exceptions and log error messages in a structured way.\n",
      "The FileHandler class is responsible for handling file-related operations in the repository agent. It provides methods to read and write file content, retrieve code information for a given object, generate the file structure, and convert the file content to markdown format. The class has attributes for the repository path, file path, and project hierarchy. The class methods include reading file content, retrieving code information, writing file content, getting modified file versions, retrieving end line numbers, adding parent references to nodes in the AST, getting functions and classes in code content, generating file structures, generating the overall structure of the repository, and converting file content to markdown format. The class also provides an output example and a description of the init function.\n",
      "The provided code includes four functions: read_file, get_obj_code_info, write_file, and get_modified_file_versions. \n",
      "\n",
      "The read_file function reads the content of a specified file using the repo_path and file_path attributes of the object. It is called within the Runner class to process file changes.\n",
      "\n",
      "The get_obj_code_info function retrieves detailed information about a specific code object within a file. It takes input parameters such as code type, code name, line numbers, parameters, and an optional file path. It is used in other parts of the project to gather information about functions and classes within a file.\n",
      "\n",
      "The write_file function writes the provided content to a file specified by the file path. It ensures that the file path is relative and creates directories if necessary. It is called within the Runner class to write markdown content to .md files.\n",
      "\n",
      "The get_modified_file_versions function retrieves the current and previous versions of a modified file. It initializes a Git repository object, reads the current version of the file, and retrieves the previous version from the last commit in the repository.\n",
      "The project involves several functions that analyze code content. The \"get_new_objects\" function compares current and previous versions of a .py file to extract added and deleted objects. The \"get_end_lineno\" function retrieves the end line number of a given node in the code's Abstract Syntax Tree (AST). The \"add_parent_references\" function adds a parent reference to each node in the AST. The \"get_functions_and_classes\" function retrieves functions, classes, their parameters, and hierarchical relationships from the code content by parsing it into an AST. These functions are used to analyze the code structure and hierarchy.\n",
      "The given code contains three functions: get_functions_and_classes, generate_file_structure, and generate_overall_structure. \n",
      "\n",
      "The get_functions_and_classes function extracts information about functions and classes from the code content and returns a list of tuples containing details about these objects. \n",
      "\n",
      "The generate_file_structure function takes a file path as input, reads the file content, and uses the get_functions_and_classes function to extract functions and classes from the code. It then generates the file structure by retrieving detailed information about the code objects and returns a list of dictionaries representing the file structure. \n",
      "\n",
      "The generate_overall_structure function retrieves file information from a target repository, excluding specified files, and uses the AST-walk method to obtain all objects. It generates the file structure for each file using the generate_file_structure function and stores the structures in a dictionary. \n",
      "\n",
      "Overall, these functions provide functionality to extract and generate file structures from code content and repositories.\n",
      "The first paragraph describes the generate_overall_structure function, which is used to initialize the meta information of a repository by generating the file structure for each file. The function takes in parameters such as file_path_reflections and jump_files, and returns a dictionary called repo_structure.\n",
      "\n",
      "The second paragraph describes the convert_to_markdown_file function, which converts the content of a file to markdown format. It takes in a file_path parameter and reads the project hierarchy JSON file to retrieve information about the file. It then processes the file structure data to generate markdown content based on the hierarchy of objects in the file. The function updates the JSON data with any changes and regenerates the markdown documentation for the file.\n",
      "The code includes two functions: \"add_new_item()\" which adds new projects to a JSON file and generates documentation, and \"process_file_changes()\" which processes changed files based on their file path, including new and existing files.\n",
      "The InterceptHandler class is used to intercept standard logging messages and redirect them to Loguru for processing. It overrides the emit method to retrieve the log level and caller information, and then logs the message using Loguru. The set_logger_level_from_config function utilizes InterceptHandler to set the logger level and intercept standard logging messages. This allows for seamless integration of Loguru logging with standard logging in Python applications. The emit function logs a message using Loguru based on the provided log record, while the set_logger_level_from_config function sets the logger level and intercepts standard logging messages. Developers can use these functions to enhance logging capabilities and adjust logging behavior based on configuration settings.\n",
      "The \"language_prompt\" function prompts the user to enter a language and returns the corresponding language name. It is used to set the language parameter for the agent's project settings. The \"cli\" function serves as a framework for generating code documentation using LLM technology. The \"configure\" function is responsible for configuring the agent's parameters, prompting the user for input and saving the settings. The \"run\" function executes the program with specified parameters.\n",
      "The given information describes the parameters and code description of the \"run\", \"clean\", and \"print_hierarchy\" functions. The \"run\" function initializes settings, updates configuration, and executes the document generation process. The \"clean\" function deletes fake files generated during the documentation process. The \"print_hierarchy\" function prints the hierarchy of the target repository.\n",
      "The given text describes two functions: print_hierarchy and chat_with_repo. \n",
      "\n",
      "The print_hierarchy function is a recursive function that prints the repository objects in a hierarchical manner. It takes optional parameters to control the printing behavior, such as whether to print the difference status of the objects and a list of file paths to be ignored. The function uses the print_indent helper function to generate the indentation string and determines the name to be printed for each object based on its item_type attribute. It checks the diff_status parameter and the result of the need_to_generate function to decide whether to print the object's item status. It recursively calls itself on the children of each object to print the hierarchy.\n",
      "\n",
      "The chat_with_repo function initializes a chat session with the repository. It determines the paths to the markdown documents folder and the project hierarchy file. If the markdown folder exists, it logs the initiation of the chat session and creates an instance of the ChatRepo class. This instance is configured with the provided parameters such as chunk size and overlap. The start_chat method of the ChatRepo class is then called to begin the interactive chat session.\n",
      "The chat_with_repo function allows users to interact with the ChatRepo functionality to obtain documentation-related information. The show_chunk function demonstrates how documents can be chunked and saved to a file for further analysis. Users need to ensure the necessary markdown documents and hierarchy file are available, and set appropriate values for chunk size and overlap.\n",
      "The Task class represents a task with a task ID, dependencies, extra information, and status. The TaskManager class manages tasks by adding, retrieving, marking as completed, and maintaining task dependencies. The TaskManager class is utilized within the TaskManager class in multi_task_dispatch.py. The init function initializes a Task object with a task ID, dependencies, and optional extra information. The TaskManager class provides methods to add tasks with dependencies, retrieve the next available task for a process, mark tasks as completed, and manage task dependencies. The get_task_manager method in the MetaInfo class utilizes the TaskManager class to manage tasks based on the topology of objects in the repository.\n",
      "The given code describes a MultiTaskDispatch object that manages tasks efficiently. It uses a task dictionary, a threading lock, and other attributes to track task IDs and queries. The init function initializes the attributes properly. The all_success function checks if all tasks in the task dictionary have been completed. The add_task function adds a new task to the task dictionary with specified dependencies. The get_next_task function retrieves the next available task for a given process ID. The mark_completed function marks a task as completed and removes it from the task dictionary.\n",
      "The mark_completed function is used to mark a specific task as completed and update its dependencies. It acquires a lock on the task dictionary, retrieves the target task using the provided task_id, and removes it from the dictionary along with any dependencies. The worker function is responsible for performing tasks assigned by the task manager. It retrieves tasks based on the process ID, handles them using a provided function, and marks them as completed. The some_function randomly sleeps for a period of time using the time.sleep() function.\n",
      "The ProjectManager class manages project-related operations such as retrieving the project structure and building a path tree. It has attributes for the repository path, project instance, and project hierarchy JSON file. The class provides methods to get the project structure by walking through the directory tree and build a path tree based on references and document item paths. The Runner class interacts with ProjectManager and other components for project management and documentation tasks. The init method initializes the ProjectManager object with the repository path and project hierarchy. The get_project_structure method retrieves the project structure by recursively traversing the directory tree. The walk_dir function is used by get_project_structure to walk through the directory structure and collect Python files. The build_path_tree method constructs a tree structure based on provided paths.\n",
      "The code description explains two functions: build_path_tree and tree_to_string. The build_path_tree function initializes a tree structure using defaultdict and creates nested nodes in the tree based on the paths provided. The tree_to_string function converts a nested dictionary representing a tree structure into a string with proper indentation. These functions are essential for organizing and visualizing relationships between paths in a project's hierarchy. The output examples demonstrate the expected output of these functions.\n",
      "The Runner class is responsible for generating and updating documentation for a target repository. It initializes various components and provides methods for generating documentation for individual objects, detecting changes in the repository, and running the document update process. The class has attributes such as the project hierarchy path, project manager, change detector, chat engine, meta info, runner lock, and summarizator. It also has methods for retrieving all Python files in a directory, generating documentation for a single object, generating all documents in the repository, writing document information in markdown format, committing changes, and running the document update process.\n",
      "The generate_doc_for_a_single_item function is responsible for generating documentation for a single object. It checks if the object needs to be generated and if it is in the ignore list. If not, it generates the documentation using the ChatEngine and FileHandler classes. The generate_doc method prepares prompts for the chat model, sends a request to generate the documentation, and handles errors. The generated documentation includes information about references and return values. The function updates the DocItem object and logs any errors. The first_generate function generates all the documents in the target repository, handles interruptions, and prints the task list.\n",
      "The function first_generate is used to generate all the documents in a target repository. It creates a task_manager object, calculates the topological order of the objects in the repository, and assigns tasks to worker threads. The markdown_refresh function is responsible for refreshing the markdown documents by writing the latest document information to the target repository. The function also handles errors, updates document version information, saves metadata, and generates a summary.\n",
      "The to_markdown function generates markdown content based on a given DocItem object and its children recursively. It constructs the content by appending markdown syntax based on the item's type, name, and content. It also handles parameters and appends the last content from the item's md_content attribute. The function then recursively calls itself for each child, appending a horizontal rule between each child's content. Once the content for a file item is generated, it is written to a .md file. After processing all file items, a message is logged indicating that the markdown documents have been refreshed. \n",
      "\n",
      "The recursive_check function checks if there is any markdown content stored in the md_content attribute of a given DocItem object. It recursively checks the children of the DocItem until a document with content is found or all children have been checked. It returns True if content is found, otherwise False. This function is crucial for traversing the hierarchical structure of DocItems to determine if any markdown content exists within the tree of objects.\n",
      "The given text includes a class definition for \"ExampleClass\" and a function definition for \"example_function\". It also mentions the presence of documentation content for both the class and the function.\n",
      "The provided code includes three functions: git_commit, add_new_item, and process_file_changes. \n",
      "\n",
      "The git_commit function commits changes to a Git repository with a specified commit message. It uses the subprocess module to execute a Git commit command and catches any errors that may occur.\n",
      "\n",
      "The add_new_item function adds new projects to a JSON file and generates corresponding documentation. It takes a file handler object and JSON data as parameters. It iterates through the functions and classes in the file, retrieves code information, generates documentation, and updates the JSON file and markdown documentation.\n",
      "\n",
      "The process_file_changes function handles changes in a file by processing its content and updating the project hierarchy and markdown documentation. It takes the repository path, file path, and a flag indicating if the file is new as parameters. It reads the file content, parses the differences using a ChangeDetector object, and stores the changes.\n",
      "The function updates the file structure information based on changes in a file. It logs the changes, updates a JSON file, converts the changes to markdown format, and writes the markdown content to a file. The update_existing_item function is used to update the existing projects by modifying the file structure information dictionary based on the changes in the file. It retrieves the new and deleted objects, processes the deleted objects, generates the current file structure information, and updates the global file structure information dictionary.\n",
      "The function \"update_object\" is responsible for generating documentation content and updating the corresponding field information of an object. It takes parameters such as a dictionary containing old object information, a file handler, the object name, and a list of object referencers. The function retrieves the object's information, generates documentation content using the ChatEngine object, and updates the object's field information with the generated content. The function handles potential errors and returns the generated documentation as a response message.\n",
      "The generate_doc function relies on the ChatEngine and ResponseMessage classes to generate documentation and store response messages. Developers can use the update_object function to automatically generate detailed documentation for code objects. However, the generate_doc function may have limitations in processing code that exceeds the model's token limit. The get_new_objects function retrieves added and deleted objects by comparing current and previous versions of a .py file. It takes a file_handler object as input and performs several steps to calculate the added and deleted objects. The function is called in the update_existing_item function of the Runner class to update the file structure information dictionary.\n",
      "The summary describes two classes, LogLevel and ProjectSettings, and two functions, serialize_ignore_list and validate_language_code. \n",
      "\n",
      "The LogLevel class defines different log levels and ensures that only valid log levels are accepted. It is used in the ProjectSettings class to set the log level for the agent.\n",
      "\n",
      "The ProjectSettings class manages various project settings and includes methods for serializing and validating specific fields. It is used during the configuration process to collect and validate user input for project settings.\n",
      "\n",
      "The serialize_ignore_list function handles a list of strings and returns a modified list based on certain conditions.\n",
      "\n",
      "The validate_language_code function validates a language code input and returns the corresponding language name.\n",
      "The given information describes several functions and a class in a codebase. \n",
      "\n",
      "1. The first function, \"match_language\", uses the \"Language.match\" method to find a matching language for the input code. It handles the \"LanguageNotFoundError\" by raising a ValueError with a specific error message. An example output is provided.\n",
      "\n",
      "2. The second function, \"set_log_level\", sets the log level based on the input value provided by the user. It takes two parameters: \"cls\" (class method parameter) and \"v\" (a string representing the log level to be set). The function converts the input to uppercase, checks if it is a valid log level by verifying it against the LogLevel enum members, and returns an instance of LogLevel with the corresponding value if it is valid. Otherwise, it raises a ValueError indicating an invalid log level. The function ensures that only predefined log levels from the LogLevel enum can be set.\n",
      "\n",
      "3. The third function, \"serialize_target_repo\", converts the provided target repository path into a string representation. It takes one parameter, \"target_repo\", which represents the directory path of the target repository. The function converts the directory path into a string using the str() function and returns the string representation of the target repository path.\n",
      "\n",
      "4. The \"ChatCompletionSettings\" class defines settings related to chat completion functionality. It has attributes such as \"model\", \"temperature\", \"request_timeout\", \"base_url\", and \"openai_api_key\". The class inherits from BaseSettings and includes a method \"serialize_base_url\" that serializes the base URL to a string. The class is instantiated in the configure function of main.py to allow users to set and save chat completion settings interactively. The settings are used to create a Setting instance that combines project and chat completion settings for configuration. In the run function of main.py, ChatCompletionSettings is instantiated with parameters passed to the function, along with other project settings. The resulting Setting instance is used to write the configuration and run the documentation generation process.\n",
      "\n",
      "5. The last function, \"serialize_base_url\", converts the provided base URL to a string format. It takes one parameter, \"base_url\", which represents the base URL that needs to be serialized. The function converts the base URL to a string format using the str() function.\n",
      "\n",
      "The provided examples demonstrate the usage and expected outputs of these functions and the class.\n",
      "The Setting class is used to define and manage project-specific configurations and chat completion settings. It includes attributes for project and chat completion, which store instances of ProjectSettings and ChatCompletionSettings classes respectively. The Setting class is used in the configure and run functions of main.py to collect and save user-defined settings, and to initialize project and chat completion settings. Developers can customize these settings to generate documentation tailored to their requirements.\n",
      "The Summarizator class provides methods for summarizing a set of documents and generating a consolidated summary. It initializes with a path to the document directory and a model name. The class has methods for splitting documents into chunks, reading markdown files, and generating a consolidated summary. It relies on external libraries and classes for its operations. The class is called in the Runner class to summarize markdown documents. The init method initializes the object with the provided path and model name. The get_map_reduce_chain method sets up a chain of operations for mapping and reducing the documents. The split_documents method splits a document into chunks based on specified parameters. The read_md_files method reads markdown files from a specified root path. The get_first_summarization method reads markdown files, splits them into chunks, sets up the map-reduce chain, and generates a consolidated summary.\n",
      "The given information describes two functions in a project related to document summarization. \n",
      "\n",
      "The first function, read_md_files, reads all Markdown files from a specified root path and returns a list of documents. It iterates through subdirectories, loads Markdown files, and appends them to a list. This function is called by the get_first_summarization method in the Summarizator class.\n",
      "\n",
      "The second function, get_first_summarization, generates a summary of the documents based on a map-reduce chain. It reads Markdown files, splits the documents into smaller chunks, and processes them using a map-reduce chain. This function is called by the first_generate and run methods in the Runner class.\n",
      "\n",
      "The final consolidated summary captures the main points and key details from each document.\n",
      "The ChatRepo class provides automatic question and answer functionality based on specific and general models for chat interactions. It has attributes for the model path, hierarchy path, model name, chunk size, and chunk overlap. The class has methods for retrieving answers based on question classification and for initiating a chat interaction loop. When called in the project, the ChatRepo instance is created with the provided parameters and the start_chat method is invoked to enable a chat session with automated responses.\n",
      "The given code is for a ChatRepo object that handles chat language chain operations. The \"init\" function initializes the object with specific models for handling chat operations. The \"get_answer\" function classifies a user's question and retrieves an answer based on the classification. The \"start_chat\" function initiates a chat session where users can input questions and receive real-time answers from the chatbot.\n",
      "The get_answer method needs to be implemented and accessible in the ChatRepo class for smooth question classification and response generation. The start_chat function is crucial for driving the conversation flow based on user input.\n",
      "The ClassificationModel class is used for handling classification tasks in a chat language chain. It has attributes such as history, methods, contextualize_q_prompt, and prompt. The class provides methods for classifying user questions, setting prompts, managing chat history, generating standalone questions, and setting classification prompts. It plays a crucial role in the chat language chain by enabling the classification of user questions and providing appropriate responses. The init function initializes a ClassificationModel object with specific attributes and prompts. The get_methods_from_hierarchy function extracts method names from a hierarchy dictionary. The get_classification function classifies a user's question as specific or general.\n",
      "The get_classification function is a crucial part of the ClassificationModel class. It classifies the user's question and handles it accordingly. If the classification is 'specific', it prints the classification and returns it. Otherwise, it generates a standalone question and prompts the user for a response. The function then extracts the classification from the response and returns it. The classify_trough_name method is used to check if any method from a list of methods is present in the input question. The __set_contextualize_prompt method sets up a contextualized question prompt by combining system prompts and user input within a chat context. The __add_to_history method adds user input and system output to the history list with a maximum length constraint. The __generate_standalone_question method generates a standalone question and updates the history list.\n",
      "The __generate_standalone_question function generates a standalone question based on user input using an LLMChain model. It updates the conversation history and returns the standalone question. The __set_classification_prompt function sets up a prompt template for classification prompts based on a list of examples. It assigns the prompt to the object's prompt attribute. Both functions are essential for the ClassificationModel class.\n",
      "The GeneralModel class is a specialized model for chat language chain operations. It has attributes such as path, path_hierarchy, model_name, template, chain, and docs. The class extends the Model class and initializes with specific parameters. It sets up a template for contextualizing questions, loads documents, and creates a runnable chain for chat interactions. The GeneralModel class is called within the ChatRepo class's initialization. The init function initializes the GeneralModel object and sets up essential attributes. The set_chain function initializes a chat processing chain. The load_docs function loads documents from a specified directory.\n",
      "The load_docs function is called within the init method of the GeneralModel class, allowing documents to be automatically loaded when an instance of GeneralModel is created. This ensures that subsequent operations that rely on these documents can be carried out smoothly. However, it is important to make sure that the provided path is correct and contains the required documents in the correct format for successful loading.\n",
      "The ClassDef Model is a foundational class for creating chat language chain models. It initializes attributes such as path, llm, docs, prompt, store, vectorstore, chain, and hierarchy. It provides methods for retrieving the prompt, setting up a vector store, retrieving session history, getting the chat language chain, creating a chat prompt template, and splitting documents into chunks. The Model class is used as a base for more specialized models and requires necessary parameters for effective functionality. The init function initializes the Model class with specific attributes and objects. The get_prompt function retrieves and returns the prompt stored in the object. The set_vectorstore function creates a vector store for a list of documents by splitting them into chunks.\n",
      "The provided information explains the purpose and functionality of the get_session_history function. It retrieves the chat history for a specific session and creates a new session if it does not exist. The function requires a valid session_id as a parameter and returns the chat history for the specified session. It is used within the Model class to create a RunnableWithMessageHistory object for processing chat messages.\n",
      "The function \"get_session_history\" returns the session ID and chat history of a session. The chat history includes messages and their timestamps. \n",
      "\n",
      "The function \"create_chat_prompt\" generates a chat prompt template based on a provided system prompt. The template includes the system prompt, a placeholder for chat history, and a placeholder for human input.\n",
      "\n",
      "The function \"create_runnable_chain\" constructs a chain for processing chat messages by combining prompts and a retriever. It initializes a chat prompt template, creates a history-aware retriever, generates prompts for question-answer interactions, and constructs a retrieval chain. It returns a RunnableWithMessageHistory object that encapsulates the chat message processing chain.\n",
      "The RunnableWithMessageHistory class is defined with three optional parameters: input_messages_key, history_messages_key, and output_messages_key. The get_chunk_docs function is used to split a list of documents into chunks of text with a specified size and overlap. It utilizes the get_chunk_with_source function from utilities.py. The function takes the input list of documents and returns the split chunks with assigned source metadata. It is called within the show_chunk function in main.py to display and save the chunked content of the documents. The output of get_chunk_docs can be further processed or saved for analysis or storage purposes.\n",
      "The SpecificModel class is a specialized model for chat language chain operations with specific functionalities. It loads documents, sets up a vector store, and creates a retriever for self-querying. It is used in the chat language chain project to create a specific model instance for chunking documents. The init function initializes the SpecificModel object by setting up attributes, loading documents, configuring a vector store, creating a retriever object, and establishing a chat message processing chain. The set_chain function initializes the chat message processing chain by creating prompt templates and setting up a retrieval chain.\n",
      "The project structure includes the set_chain function, which is called within the init method of the SpecificModel class. This function sets up the entire model, loads documents, configures vector stores, initializes metadata field information, creates a retriever object, and initializes the chat message processing chain. The load_docs function is responsible for loading documents from a specified directory and storing them in the object's 'docs' attribute. The get_docs function retrieves and returns the value of the 'docs' attribute from the object.\n",
      "The given code description provides information about four functions: split_documents, get_chunk_with_source, filter_docs, and get_json_from_path. \n",
      "\n",
      "1. split_documents: This function splits a document into chunks of text based on specified parameters such as chunk size and overlap. It uses headers and a MarkdownHeaderTextSplitter to split the document and returns the splits.\n",
      "\n",
      "2. get_chunk_with_source: This function splits a list of documents into chunks of text and assigns a source metadata to each chunk based on the document's filename. It calls the split_documents function for each document, assigns metadata, and returns the aggregated splits.\n",
      "\n",
      "3. filter_docs: This function filters a list of documents based on the presence of \"summary.md\" in the metadata source field. It selects only the documents where \"summary.md\" is found and returns a new list with the filtered documents.\n",
      "\n",
      "4. get_json_from_path: This function loads and returns JSON data from a specified file path. It opens the file, loads the JSON data, and returns it. It is used in the Model class to load JSON data from a specified path_hierarchy file.\n",
      "\n",
      "Each function has its own parameters, code description, and output example.\n",
      "The get_qa_system_prompt function retrieves a prompt template for question-answering tasks related to code documentation files. It is called within the set_chain method of the SpecificModel class to create a runnable chain for question-answering tasks. The get_contextualize_q_system_prompt function formulates a standalone question from a given chat history and the latest user question, ensuring it can be understood without the context of the chat history. It is used in different parts of the project to set up the chain and generate standalone questions.\n",
      "The GradioInterface class is used to create a user interface for interacting with a chatbot system. It has attributes for handling responses, CSS styling, and methods for setting up the interface, formatting and displaying responses, and cleaning the interface. The class is called in the main function to create the interface. The init function initializes the GradioInterface object with the respond function and CSS styling templates. The wrapper_respond function formats the outputs of the respond function with markdown and CSS styling.\n",
      "The provided code includes three functions: clean, setup_gradio_interface, and respond_function. \n",
      "\n",
      "The clean function generates HTML outputs for different sections of the Gradio interface, such as \"Response\", \"Embedding Recall\", and \"Code\". It constructs HTML content using predefined CSS styles and returns these outputs along with an empty message and code snippet. It is called within the setup_gradio_interface function and can be used to reset the displayed outputs on the Gradio interface.\n",
      "\n",
      "The setup_gradio_interface function initializes a Gradio interface for users to interact with the chat system. It defines input elements such as textboxes and buttons for user input and interaction. It formats the output sections for response, embedding recall, and code display using HTML and CSS styling. It links the input elements to the wrapper_respond function for handling user inputs and displaying formatted outputs on the interface. The clean function is linked to a ClearButton element to reset the input fields and displayed outputs.\n",
      "\n",
      "The respond_function processes a message and returns it along with additional outputs such as \"Embedding_recall_output\", \"Key_words_output\", and \"Code_output\". It takes a message and system information as input parameters and includes a placeholder string for further processing or information storage.\n",
      "The JsonFileProcessor class is designed to process JSON files, extract specific data, and search for code contents based on given criteria. It provides methods to read JSON files, extract data, and search for code contents within the JSON data. The class is efficient and allows users to extract specific data and search for code contents seamlessly. The class also handles exceptions such as FileNotFoundError and JSONDecodeError appropriately. The init function initializes the object with a file path, while the read_json_file function reads JSON data from a file specified by the file_path attribute. The extract_data function loads JSON data, iterates through it, extracts specific information, and builds dictionaries based on the extracted content. The extracted data is then used for further processing.\n",
      "The provided information describes two functions: recursive_search and search_code_contents_by_name. \n",
      "\n",
      "The recursive_search function is used to search for a specific text within nested dictionaries and lists, extracting relevant data based on the search criteria. It iterates over the elements of the data_item, checking for matches based on the search_text. If a match is found, the corresponding code_content and md_content are appended to the code_results and md_results lists, respectively. The function handles nested dictionaries and lists by making recursive calls to itself. \n",
      "\n",
      "The search_code_contents_by_name function attempts to read a JSON file and search for occurrences of a specified search_text within the JSON data. If matches are found, the function extracts the corresponding code content and markdown content and returns them as lists. This function relies on the recursive_search method to navigate through nested data structures within the JSON file. \n",
      "\n",
      "Both functions require specific parameters to be provided correctly and handle exceptions appropriately.\n",
      "The main function serves as the entry point for the project. It initializes variables and objects, extracts data from JSON files, and stores the data in Chroma for further processing.\n",
      "The TextAnalysisTool class provides various text analysis functionalities such as keyword extraction, tree structure generation, formatting chat prompts, searching code blocks, converting search results to Markdown format, and extracting relevant class or function names. It is initialized with an OpenAI language model and a database path. The class has methods for generating keywords, creating tree structures, formatting chat prompts, searching code blocks, converting search results to Markdown format, and extracting relevant class or function names. The class is utilized in the project by the RepoAssistant class and is tested in the TestTextAnalysisTool class. The class requires proper initialization with the required dependencies before using its methods.\n",
      "The given information describes three functions: format_chat_prompt, queryblock, and list_to_markdown. \n",
      "\n",
      "The format_chat_prompt function generates a formatted prompt message for a chat conversation by combining the user's message, system's instruction, and a placeholder for the assistant's response. It is used in the respond method of the RepoAssistant class.\n",
      "\n",
      "The queryblock function searches for specific text within a JSON file and retrieves matching code content and markdown content. It acts as a mediator between the user input and the search functionality.\n",
      "\n",
      "The list_to_markdown function converts a list of items into a Markdown formatted string with each item numbered. It is used to convert a list of unique code snippets into Markdown format for display in the response message generated by the respond method.\n",
      "\n",
      "The nerquery function extracts the most relevant class or function based on specific instructions by constructing a query and using the llm.complete method to retrieve a response.\n",
      "The function is called within the respond method of the RepoAssistant class in the rag.py file. It extracts keywords from the bot message and prompt questions using the nerquery function. These keywords are used to query blocks of code, and the retrieved documents are processed to generate a response. The input message should be in the correct format to receive meaningful output.\n",
      "The RepoAssistant class is designed to assist with repository question and answer tasks by utilizing various AI models and tools. It has attributes such as API key, API base URL, and database path. The class includes methods for generating search queries, ranking documents, providing answers, converting lists to markdown format, generating answers based on related code and documents, and processing user messages. The class interacts with OpenAI models, text analysis tools, and database processors to provide accurate and detailed responses. The __init__ function initializes the RepoAssistant object with the provided API key, API base URL, and database path, as well as initializes other necessary components.\n",
      "The given code description provides an overview of three functions in the RepoAssistant class: generate_queries, rerank, and rag. \n",
      "\n",
      "The generate_queries function takes an input query string and generates multiple search queries based on it. It constructs a prompt template, uses a language model to complete the prompt, and returns a list of the generated queries.\n",
      "\n",
      "The rerank function sorts a list of documents based on their relevance scores and returns the top 5 most relevant document contents. It sends a request to a language model to rank the documents and retrieves the relevance scores from the response.\n",
      "\n",
      "The rag function generates a response for a given query by combining the query and retrieved documents. It passes the combined information to a language model to generate a response and returns the generated response.\n",
      "\n",
      "These functions are called within the respond method of the RepoAssistant class to generate search queries, refine the list of documents based on relevance scores, and generate a response based on the user's query and retrieved documents.\n",
      "The given code description provides an overview of three functions: list_to_markdown, rag_ar, and respond. \n",
      "\n",
      "The list_to_markdown function converts a list of items into a markdown formatted string with numbered list items. It takes a list as input and returns a markdown formatted string.\n",
      "\n",
      "The rag_ar function generates a response for a Repository-Level Software Q&A assistant based on the user's query, related code snippets, documents, and the project name. It constructs a message system and uses a language model to generate a response incorporating the provided information.\n",
      "\n",
      "The respond function processes a user message and an instruction, generates questions using the keyword function, retrieves relevant code documents, and uses the RAG model to generate a response based on the retrieved documents and the user's query.\n",
      "The \"respond\" function takes a user message and a system instruction as input. It formats the chat prompt, generates keywords and queries, retrieves relevant code documents, reranks them, generates a response using the RAG model, and extracts relevant keywords from the bot message and prompt questions. Finally, it returns the user's message, bot message, markdown formatted list of retrieved documents, generated questions, unique code snippets, and markdown formatted code blocks.\n",
      "The ChromaManager class is used to manage collections in ChromaDB. It initializes a collection and creates a vector store. The class has attributes for the API key, API base, the collection being managed, and a flag indicating if the collection is new. The init method initializes the ChromaManager object with the API key and base URL, and calls the init_chroma_collection method to set up the collection. The init_chroma_collection method either loads an existing \"test\" collection or creates a new one. The create_vector_store method processes Markdown content and stores it in the collection. The ChromaManager class is utilized by the RepoAssistant class to manage Chroma data.\n",
      "The create_vector_store function checks if it is a new collection and generates ids based on the length of the data. It adds the documents and metadata to the Chroma collection using the generated ids. In the main function, it is called after extracting data and before initializing the GradioInterface. In the test scenario, it is tested by mocking the embedding function and asserting the expected calls to the embedding function and the collection's add method. It is important to ensure compatible lengths of data and understand the data processing and storage flow in Chroma for effective use of the function.\n",
      "The GitignoreChecker class checks files and folders in a specified directory against patterns defined in a .gitignore file. It initializes with a directory and .gitignore file path, loads and parses the file, and provides methods to check files and folders against the patterns. It is used in the FileHandler class to exclude certain files from processing. The init function initializes the GitignoreChecker object with the directory and gitignore_path attributes. The _load_gitignore_patterns function loads and parses the .gitignore file, categorizing the patterns into folder and file patterns. The _parse_gitignore function extracts patterns from the .gitignore file.\n",
      "The project involves parsing and loading a .gitignore file, extracting patterns from it, and categorizing them into folder and file patterns. The _split_gitignore_patterns function is used to separate the patterns, while the _is_ignored function checks if a given path matches any of the patterns. The check_files_and_folders function uses these functions to filter out ignored files and folders in a specified directory, returning a list of files that are not ignored and have the '.py' extension.\n",
      "The make_fake_files function detects staging area information based on git status and performs specific actions on different types of files. It returns a dictionary mapping original file paths to fake file paths and a list of jump files to skip during parsing and documentation generation. The delete_fake_files function removes all fake files after the task execution is completed. The gci function is used within delete_fake_files to traverse files in a specified filepath and perform file operations based on certain conditions. It is important to call delete_fake_files before using make_fake_files to ensure the integrity of the document generation process.\n",
      "The gci function lists all files in a specified filepath and performs various operations on them. If a file ends with a specific substring, it modifies the file path, deletes the original file, and performs additional actions based on the file size. Caution should be exercised when using this function as it involves file deletion and renaming operations.\n",
      "The TestChangeDetector class is a unit test class that tests the functionality of the ChangeDetector class methods. It sets up a test environment by creating a test repository, initializing a Git repository, configuring Git user information, creating test files, and simulating Git operations. The test_get_staged_pys function tests the get_staged_pys method of ChangeDetector by creating a new Python file, staging it, and checking if it is in the staged files list. The test_get_unstaged_mds function tests the get_to_be_staged_files method of ChangeDetector by modifying a Markdown file without staging it and checking if it appears in the unstaged files list.\n",
      "The given text describes three functions: test_get_unstaged_mds, test_add_unstaged_mds, and tearDownClass. \n",
      "\n",
      "The test_get_unstaged_mds function is a unit test that verifies the behavior of the get_to_be_staged_files method in the ChangeDetector class. It modifies a Markdown file without staging the changes and checks if the modified file is correctly identified as an unstaged file.\n",
      "\n",
      "The test_add_unstaged_mds function is a unit test that verifies the behavior of the add_unstaged_files method in the ChangeDetector class. It first calls the test_get_unstaged_mds method to ensure the presence of unstaged Markdown files. Then, it adds the unstaged files to the staging area and checks if all unstaged files have been successfully added.\n",
      "\n",
      "The tearDownClass function is responsible for cleaning up the test repository after all tests have been executed. It closes the repository and removes the test repository path to ensure proper cleanup of temporary files or resources.\n",
      "The TestGradioInterface class is a unit test class that tests the functionality of the GradioInterface class. It sets up necessary objects for testing, such as a MagicMock object for mocking the respond function and an instance of the GradioInterface class. The class has methods to test the setup of the Gradio interface and the integration of the respond function. The setup_gradio_interface method initializes the Gradio interface with input fields and buttons for user interaction. The test_respond_function_integration method ensures that the respond function is integrated correctly by calling it with test messages and asserting that the mock_respond_function is called with the correct parameters. The setUp function initializes the necessary objects for setting up the Gradio interface. The test_setup_gradio_interface function tests the setup of the Gradio interface by asserting that the MockBlocks object is called during the setup process.\n",
      "The respond function in the GradioInterface class must be properly integrated and invoked with the correct parameters to ensure the system's functionality and reliability.\n",
      "The TestJsonFileProcessor class is used to test the methods of the JsonFileProcessor class for reading JSON files, extracting specific data, and searching for code contents. It contains test methods that validate the functionality of the JsonFileProcessor class. The test methods utilize the unittest framework and the @patch decorator to mock certain functionalities. The setUp function initializes the JsonFileProcessor object with a specified JSON file path. The test_read_json_file method tests the read_json_file method by checking if it reads the JSON file correctly and returns the expected data. The test_extract_md_contents method verifies the extract_md_contents method by ensuring that it extracts markdown contents as intended. The test_search_in_json_nested method tests the search_in_json_nested method to validate the search functionality within nested JSON data. The TestJsonFileProcessor class plays a crucial role in ensuring the correctness and reliability of the methods implemented in the JsonFileProcessor class.\n",
      "This is a unit test function that tests the search_in_json_nested method of a processor object. It uses a mock object to simulate reading JSON data and ensures that the mock object is properly configured. The expected output is a list containing the markdown content \"content1\". The function calls the search_in_json_nested method with specific parameters and asserts the result. It also checks that the mock_file object is called with specific arguments.\n",
      "The TestYourScript class contains two test methods, test_load_config() and test_main(), which are used to test the functionality of the main script by mocking responses and checking certain conditions. The test_load_config() method tests the loading of a configuration file by mocking the open function and asserting the values of specific keys in the loaded configuration. The test_main() method mocks different components of the script, sets up mock responses, executes the main function, and verifies if certain components are initialized correctly. It is important to maintain the integrity of the mocked responses and assertions to accurately test the behavior of the main script.\n",
      "The TestTextAnalysisTool class is used to test the methods of the TextAnalysisTool class for various text analysis functionalities. It sets up necessary mocks and patches, initializes the TextAnalysisTool with mocked dependencies, and tests the methods to ensure their proper functionality. The class contains test methods for keyword extraction, tree structure generation, chat prompt formatting, code block searching, and class/function name extraction. The setUp function initializes the necessary mocks and patches, while the tearDown function stops the applied patches. The test_keyword function validates the keyword extraction functionality, and the test_tree function tests the tree structure generation functionality.\n",
      "The first summary describes a test function that sets up a mock response and calls the tree function of the TextAnalysisTool class to generate a tree structure. It asserts that the returned tree structure matches the expected value, ensuring the correct functioning of the tree generation feature.\n",
      "\n",
      "The second summary describes a function that generates a formatted prompt message for a chat conversation. It takes a user message and a system instruction, constructs the prompt message, and returns it. It is used in the respond method of the RepoAssistant class to generate prompts for chat conversations.\n",
      "\n",
      "The third summary describes a test function that sets up a mock response and calls the queryblock method of the TextAnalysisTool class. It asserts that the returned result matches the expected code content. The queryblock method searches for specific text within a JSON file and retrieves matching code content and markdown content.\n",
      "\n",
      "The fourth summary describes a test function that validates the functionality of the nerquery method in the TextAnalysisTool class. It sets up a mock response for the llm.complete method, calls the nerquery method with a test message, and compares the obtained function name with the expected value. It also checks if the debug method of the logger manager is called. The nerquery method constructs a query based on specific instructions and returns the obtained response from the completion of the query.\n",
      "The TestRepoAssistant class is a unit test class that tests the functionality of the RepoAssistant class by setting up mocks for external dependencies and patching external classes. It includes test methods to validate the functionality of the RepoAssistant class, such as generating queries, performing RAG analysis, extracting and formatting documents, and responding to queries. The class also includes setup and teardown methods to initialize and stop patches for external classes. The setup method creates mock objects for external dependencies and patches the external classes, while the teardown method stops the patches. The class relies on mocking external dependencies for isolated testing and requires correct setup of mock responses to simulate different scenarios for testing.\n",
      "The given summary describes four test functions that are used to verify the functionality of different methods in the RepoAssistant class. These test functions set up mock responses and input parameters, call the respective methods, and check if the generated output matches the expected output. The purpose of these test functions is to ensure that the methods in the RepoAssistant class are functioning correctly and generating the expected results.\n",
      "The build_path_tree function constructs a tree structure based on provided paths and returns a string representation of the tree. It uses the who_reference_me and reference_who lists to create nodes in the tree, and processes the doc_item_path to update the tree accordingly. The tree_to_string function is used to convert the tree into a string with proper indentation. The tree function is a helper function that returns a defaultdict with the tree structure.\n",
      "The TestChromaManager class is a unit test class that tests the functionality of the ChromaManager class methods. It sets up mock objects and instances before each test method is executed. The setUp method initializes a mock ChromaDB Client and collection, and creates an instance of the ChromaManager class. The test_init method checks if the ChromaManager object is initialized correctly. The test_init_chroma_collection method tests the initialization of the chroma collection. The test_create_vector_store method tests the create_vector_store method of ChromaManager by mocking an embedding function and verifying the expected behavior. The setUp function initializes a mock client and sets up necessary mock attributes. The test_init function checks if the object is initialized correctly. The test_init_chroma_collection function tests the initialization of the Chroma collection.\n",
      "The init_chroma_collection method initializes the Chroma collection by creating a new one or loading an existing one. It handles unique constraint errors and is crucial for the proper functioning of the ChromaManager's initialization process. The test_init_chroma_collection method is used to test the initialization logic of the Chroma collection using mock clients. The test_create_vector_store function tests the create_vector_store method by mocking the embedding function and verifying the expected calls made during its execution. It validates the functionality of the create_vector_store method in processing and storing data correctly.\n"
     ]
    }
   ],
   "source": [
    "get_summary(stuff_chain, all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parallel_summary(stuff_chain, docs):\n",
    "    def process_document_with_chain(doc):\n",
    "        return stuff_chain.invoke([doc])[\"output_text\"]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Map the process_document_with_chain function to the documents\n",
    "        results = list(executor.map(process_document_with_chain, docs))\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = get_parallel_summary(stuff_chain, all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_template = \"\"\"The following is a set of summaries: {docs}\n",
    "        Please distill these summaries into a final, consolidated summary of the overall contents. Ensure the final summary captures the main points and key details from each document.\n",
    "        Helpful Answer:\n",
    "    \n",
    "        The standard format is as follows:\n",
    "\n",
    "        # title: \n",
    "        ** Project Description:**  summary of the project\n",
    "\n",
    "        Please note:\n",
    "        - Write mainly in the desired language. If necessary, you can write with some english words in the analysis and description to enhance the document's readability because you do not need to translate the function name or variable name into the target language.\n",
    "\n",
    "        \"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# title: Code Documentation Generation Project\\n\\n**Project Description:** This project focuses on generating documentation for code objects in a repository. It involves various functionalities such as copying Markdown files, creating README.md files, organizing files, analyzing file differences, identifying changes in structure, retrieving unstaged files, extracting import statements, generating documentation for functions or classes, handling errors, and managing metadata. The project utilizes libraries and tools such as GitPython, OpenAI, Gradio, and ChromaDB. The code includes classes and functions for different tasks, including file handling, text analysis, chat interactions, and document summarization. Unit tests are provided to ensure the functionality of the implemented methods. The project aims to automate the process of generating comprehensive and accurate documentation for code objects in a repository.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_chain.invoke(summary)[\"text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
