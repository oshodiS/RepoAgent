{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.58it/s]\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\reply\\Desktop\\root-cause-analysis-asset\\markdown-gpt-3.5\"\n",
    "loader = DirectoryLoader(path, glob=\"./*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(doc, chunk_size=250, chunk_overlap=30):\n",
    "    headers_to_split_on = [\n",
    "         (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text( doc.page_content)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    # Split\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "   \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = []\n",
    "for doc in docs:\n",
    "    splits = split_documents(doc)\n",
    "    for doc_split in splits:\n",
    "        filename = os.path.basename(list(doc.metadata.values())[0])\n",
    "        doc_split.metadata = {'source':filename}        \n",
    "    all_splits.extend(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature = 0.1)\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info=[\n",
    "    AttributeInfo(\n",
    "        name='source',\n",
    "        description=\"Filename and location of the source file\", \n",
    "        type=\"string\", \n",
    "    )]\n",
    "document_content_description = \"Code documentation\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = SelfQueryRetriever.from_llm(model, \n",
    "                                        vectorstore, \n",
    "                                        document_content_description, \n",
    "                                        metadata_field_info, \n",
    "                                        verbose=True,\n",
    "                                        )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    model, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks regarding code documentation file. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. It's also specified the name of the file that contains the functions.  If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "specific_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain({\"explain ref_rules.md\"},return_only_outputs=False)\n",
    "# chain({\"what is the rule class doing?\"},\n",
    "#       return_only_outputs=False)\n",
    "# chain({\"in which file is computeAccuracyStats?\"}, return_only_outputs=False)\n",
    "# chain({\"explain ref_rule_extraction_algorithm.md\"}, return_only_outputs=False)\n",
    "# chain({\"how should be created a hillclimb object?\"},\n",
    "#       return_only_outputs=False)\n",
    "# chain({\"where is ruleScore used? in which file?\"}, return_only_outputs=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 86.83it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = DirectoryLoader(path, glob=\"./summary.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hypothetical Project: Rule-Based Classification System\\n\\nProject Overview\\n\\nThe purpose of this project is to develop a rule-based classification system that can evaluate the performance of different rule sets against true labels. The project aims to provide a set of functions that calculate various metrics such as rule scores, accuracy, and fault accuracy. The system incorporates mechanisms to penalize rule length to ensure that simpler, more generalizable rules are favored.\\n\\nProject Goal\\n\\nThe primary goal of this project is to develop a robust and fair rule-based classification system. By incorporating different metrics and allowing for rule length penalization, the system aims to balance accuracy and simplicity in rule-based models. This approach ensures that the generated rules are not only accurate but also generalizable and easy to interpret.', metadata={'source': 'C:\\\\Users\\\\reply\\\\Desktop\\\\root-cause-analysis-asset\\\\markdown-gpt-3.5\\\\summary.md'})]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following context:\n",
    "{context}\n",
    "Provide a comprehensive overview of the project considering the question:\n",
    "{question}\n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = split_documents(doc[0], chunk_size=1000, chunk_overlap=100)\n",
    "for s in splits:\n",
    "    filename = os.path.basename(list(doc[0].metadata.values())[0])\n",
    "    s.metadata = {'source':filename}\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chain = (\n",
    "    {\"context\": retriever , \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Explain me what the repository is about\",\n",
    "        \"answer\": \"general\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is this program doing?\",\n",
    "        \"answer\": \"general\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"which parameter should be passed to the function?\",\n",
    "        \"answer\": \"specific\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"how the method X works?\",\n",
    "        \"answer\": \"specific\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"explain file X.md\",\n",
    "        \"answer\": \"specific\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"], template=\"{answer}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how the method get_staged_pys works\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[336], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      2\u001b[0m classification \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m classification\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\prompts\\few_shot.py:153\u001b[0m, in \u001b[0;36mFewShotPromptTemplate.format\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 153\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m example_strings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    159\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\prompts\\few_shot.py:154\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 154\u001b[0m     \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m\u001b[43m}\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    155\u001b[0m ]\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m example_strings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    159\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\prompts\\few_shot.py:154\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Get the examples to use.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_examples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 154\u001b[0m     {k: \u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39minput_variables} \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    155\u001b[0m ]\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Format the examples.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m example_strings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_prompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexample) \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    159\u001b[0m ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'answer'"
     ]
    }
   ],
   "source": [
    "response = prompt.format(input=question).strip()\n",
    "classification = response.split('\\n')[0]\n",
    "classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain me what the repository is about general\\n\\n What is this program doing? general\\n\\n which parameter should be passed to the function? specific\\n\\n how the method X works? specific\\n\\n explain file X.md specific\\n\\nQuestion: how the method get_staged_pys works'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colon_position = response.find(':')\n",
    "question = response[colon_position+1:].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'how the method get_staged_pys works', 'chat_history': [], 'context': [Document(page_content='For each rule in the ruleSet, a remote prediction task is initiated, and the results are collected. The predictions are then stored in the eachRulePredictions array. If verbose is True, a progress bar is updated during the process.', metadata={'source': 'hill_climbing_par.md'}), Document(page_content='For each rule in the ruleSet, a remote prediction task is initiated, and the results are collected. The predictions are then stored in the eachRulePredictions array. If verbose is True, a progress bar is updated during the process.', metadata={'source': 'hill_climbing_par.md'}), Document(page_content='For each rule in the ruleSet, a remote prediction task is initiated, and the results are collected. The predictions are then stored in the eachRulePredictions array. If verbose is True, a progress bar is updated during the process.', metadata={'source': 'hill_climbing_par.md'}), Document(page_content='For each rule in the ruleSet, a remote prediction task is initiated, and the results are collected. The predictions are then stored in the eachRulePredictions array. If verbose is True, a progress bar is updated during the process.', metadata={'source': 'hill_climbing_par.md'})], 'answer': \"I don't know.\"}\n"
     ]
    }
   ],
   "source": [
    "print(specific_chain.invoke({\"input\":question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'how the method get_staged_pys works', 'chat_history': [HumanMessage(content='how the method get_staged_pys works'), AIMessage(content=\"I don't know.\")], 'context': [Document(page_content='For each rule in the ruleSet, a remote prediction task is initiated, and the results are collected. The predictions are then stored in the eachRulePredictions array. If verbose is True, a progress bar is updated during the process.', metadata={'source': 'hill_climbing_par.md'}), Document(page_content='For each rule in the ruleSet, a remote prediction task is initiated, and the results are collected. The predictions are then stored in the eachRulePredictions array. If verbose is True, a progress bar is updated during the process.', metadata={'source': 'hill_climbing_par.md'}), Document(page_content='For each rule in the ruleSet, a remote prediction task is initiated, and the results are collected. The predictions are then stored in the eachRulePredictions array. If verbose is True, a progress bar is updated during the process.', metadata={'source': 'hill_climbing_par.md'}), Document(page_content='For each rule in the ruleSet, a remote prediction task is initiated, and the results are collected. The predictions are then stored in the eachRulePredictions array. If verbose is True, a progress bar is updated during the process.', metadata={'source': 'hill_climbing_par.md'})], 'answer': 'The method get_staged_pys initiates a remote prediction task for each rule in the ruleSet and collects the results, storing them in the eachRulePredictions array. If verbose is True, a progress bar is updated during the process. This functionality allows for staged predictions to be obtained and stored efficiently.'}\n"
     ]
    }
   ],
   "source": [
    "if classification == \"general\":\n",
    "    print(general_chain.invoke(question))\n",
    "else:\n",
    "    print(specific_chain.invoke({\"input\":question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    "    ))[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
