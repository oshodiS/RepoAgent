{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation Conversational Rag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.46s/it]\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\reply\\Desktop\\testREPO\\TestRepo\\markdown_docs\"\n",
    "loader = DirectoryLoader(path, glob=\"./*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature = 0.1)\n",
    "metadata_field_info=[\n",
    "    AttributeInfo(\n",
    "        name='source',\n",
    "        description=\"Filename and location of the source file\", \n",
    "        type=\"string\", \n",
    "    )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(doc, chunk_size=250, chunk_overlap=30):\n",
    "    \"\"\" Split a document into chunks of text.\"\"\"\n",
    "\n",
    "    headers_to_split_on = [\n",
    "         (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text( doc.page_content)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    # Split\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "   \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_with_source(docs, chunk_size=250, chunk_overlap=30):\n",
    "    all_splits = []\n",
    "    for doc in docs:\n",
    "        splits = split_documents(doc, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        for doc_split in splits:\n",
    "            filename = os.path.basename(list(doc.metadata.values())[0])\n",
    "            doc_split.metadata = {'source':filename}        \n",
    "        all_splits.extend(splits)\n",
    "    return all_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Explain me what the repository is about\",\n",
    "        \"answer\": \"\"\"general\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is this program doing?\",\n",
    "        \"answer\": \"\"\"general\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the project work?\",\n",
    "        \"answer\": \"\"\"general\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"which parameter should be passed to the function?\",\n",
    "        \"answer\": \"\"\"specific\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what does the function X rerurns?\",\n",
    "        \"answer\": \"\"\"specific\"\"\",\n",
    "    }\n",
    "    {\n",
    "        \"question\": \"show me how to use function X\",\n",
    "        \"answer\": \"\"\"specific\"\"\",\n",
    "    }\n",
    "    {\n",
    "        \"question\": \"how the method X works?\",\n",
    "        \"answer\": \"\"\"specific\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"\"\"explain file X.md\"\"\",\n",
    "        \"answer\": \"\"\"specific\"\"\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"], template=\"{answer}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    OpenAIEmbeddings(),\n",
    "    Chroma,\n",
    "    k=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'general\\n\\nQuestion: how does the project work'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(input=\"how does the project work\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileter the array of document based on source\n",
    "def filter_docs(docs, source):\n",
    "    return [doc for doc in docs if \"summary.md\" in doc.metadata['source']]\n",
    "summary_doc = filter_docs(docs, 'summary.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_splits = get_chunk_with_source(summary_doc, chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_general = Chroma.from_documents(documents=summary_splits, embedding=OpenAIEmbeddings(), collection_name=\"general\")\n",
    "document_content_description = \"Code documentation general summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following context:\n",
    "{context}\n",
    "Provide a comprehensive overview of the project considering the question:\n",
    "{question}\n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_retriever = vectorstore_general.as_retriever()\n",
    "general_chain = (\n",
    "    {\"context\": general_retriever , \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Code documentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_splits = get_chunk_with_source(docs)\n",
    "vectorstore_specific = Chroma.from_documents(documents=specific_splits, embedding=OpenAIEmbeddings(), collection_name=\"specific\")\n",
    "\n",
    "specific_retriever = SelfQueryRetriever.from_llm(model, \n",
    "                                        vectorstore_specific, \n",
    "                                        document_content_description, \n",
    "                                        metadata_field_info, \n",
    "                                        verbose=True,\n",
    "                                        )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Get the chat history for a session. If the session does not exist, create it.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define System Prompts\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood without the chat history. \"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks regarding code documentation file. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. It's also specified the name of the file that contains the functions. \"\n",
    "    \"If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create Prompt Templates\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create History Aware Retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    model, specific_retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Create Question Answering Chain\n",
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "\n",
    "# Create RAG chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "# Create Specific Chain with Message History Management\n",
    "specific_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question,prompt, specific_chain, general_chain):\n",
    "    response = prompt.format(input=question).strip()\n",
    "    classification = response.split('\\n')[0]\n",
    "    print(classification)\n",
    "    if classification == \"specific\":\n",
    "        answer = specific_chain.invoke({\"input\":question},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"abc123\"}\n",
    "        },  # constructs a key \"abc123\" in `store`.\n",
    "        )[\"answer\"]\n",
    "    else:\n",
    "        answer = general_chain.invoke(question)\n",
    "        \n",
    "    return answer\n",
    "\n",
    "def paginate_text(text, page_size=500):\n",
    "    return [text[i:i + page_size] for i in range(0, len(text), page_size)]\n",
    "\n",
    "def print_answer(paginated_response):\n",
    "  for i, page in enumerate(paginated_response):\n",
    "    print(f\"Page {i + 1}/{len(paginated_response)}: {page}\")\n",
    "    if i < len(paginated_response) - 1:\n",
    "            input(\"Press Enter to continue to the next page...\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: which parameters does get_staged_pys need?\n",
      "\n",
      "specific\n",
      "Page 1/1: The file that contains the function get_staged_pys is not provided.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = input(\"Enter your query (or 'exit' to quit): \")\n",
    "if query == \"\":\n",
    "    print(\"Please enter a query\")\n",
    "    query = input(\"Enter your query (or 'exit' to quit): \")\n",
    "else:\n",
    "    print(f\"Question: {query}\\n\")\n",
    "    response = get_answer(query, prompt, specific_chain, general_chain)\n",
    "    paginated_response = paginate_text(response)\n",
    "    print_answer(paginated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'foo'}|{'chat_history': [HumanMessage(content='foo'), AIMessage(content=\"I don't have enough information to provide an accurate answer.\")]}|{'context': [Document(page_content='Code Description:', metadata={'source': 'utilities.md'}), Document(page_content='Code Description:', metadata={'source': 'rules_metrics_par.md'}), Document(page_content='Consolidated Summary:', metadata={'source': 'summary.md'}), Document(page_content='of Rule instances.', metadata={'source': 'rules.md'})]}|{'answer': ''}|{'answer': 'I'}|{'answer': ' don'}|{'answer': \"'t\"}|{'answer': ' have'}|{'answer': ' enough'}|{'answer': ' information'}|{'answer': ' to'}|{'answer': ' provide'}|{'answer': ' an'}|{'answer': ' accurate'}|{'answer': ' answer'}|{'answer': '.'}|{'answer': ''}|"
     ]
    }
   ],
   "source": [
    "for chunk in specific_chain.stream({'input': 'foo'}, {'configurable': {'session_id': 'ab12'}}\n",
    "):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
