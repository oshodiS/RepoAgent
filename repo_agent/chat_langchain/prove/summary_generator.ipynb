{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a summary starting from all the previously created documentation file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\reply\\\\RepoAgent\\\\markdown_docs\"\n",
    "loader = DirectoryLoader(path, glob=\"./*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(path):\n",
    "        all_docs = []\n",
    "        abs = os.path.normpath(os.path.abspath(path))  # Normalize and convert root_path to an absolute path\n",
    "\n",
    "        for subdir, _, _ in os.walk(abs):\n",
    "            loader = DirectoryLoader(os.path.join(abs, subdir), glob=\"./*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)    \n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "        return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reply\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-3.5-turbo-1106\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import runnable sequence\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reply\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "map_prompt = hub.pull(\"rlm/map-prompt\")\n",
    "reduce_prompt = hub.pull(\"rlm/reduce-prompt\")\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map Template\n",
    "map_template = \"\"\"The following is a set of documents:\n",
    "{docs}\n",
    "Based on this list of documents, please make a short description of their contents.\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce Template\n",
    "reduce_template = \"\"\"The following is a set of summaries:\n",
    "{docs}\n",
    "Please distill these summaries into a final, consolidated summary of the overall contents. Ensure the final summary captures the main points and key details from each document.\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(doc, chunk_size=250, chunk_overlap=30):\n",
    "    \"\"\" Split a document into chunks of text.\"\"\"\n",
    "\n",
    "    headers_to_split_on = [\n",
    "         (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "    )\n",
    "    md_header_splits = markdown_splitter.split_text( doc.page_content)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    # Split\n",
    "    splits = text_splitter.split_documents(md_header_splits)\n",
    "   \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.29s/it]\n",
      "100%|██████████| 13/13 [00:03<00:00,  3.94it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  8.57it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  8.18it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.23it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00,  9.36it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = load_docs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = []\n",
    "for doc in docs:\n",
    "        splits = split_documents(doc, chunk_size=10000, chunk_overlap=0)\n",
    "        for doc_split in splits:\n",
    "            filename = os.path.basename(list(doc.metadata.values())[0])\n",
    "            doc_split.metadata = {'source':filename}        \n",
    "        all_splits.extend(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = map_reduce_chain.invoke(all_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The set of documents provided offers a comprehensive guide for developers working on a Python codebase, covering functions, classes, and testing frameworks. The documents detail tasks such as generating markdown summaries, managing metadata information, handling exceptions, and updating documentation. They also describe classes like ChangeDetector, TaskManager, and ProjectManager, along with their methods and functionalities. Proper documentation, error handling, and code structure are emphasized for efficient project management, along with unit testing frameworks for ensuring code reliability. Overall, the documents provide insights into managing tasks, handling exceptions, and generating structured documentation effectively, with a focus on functionality, testing, and best practices.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    # Example formatting: Insert newlines after each period for demonstration\n",
    "    formatted_text = a.replace(\". \", \".\\n\")\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = format_text(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a on a file\n",
    "a.replace(\". \", \".\\n\")\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    file.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_template = \"\"\"We have an existing summary that needs to be updated based on new information.\n",
    "                        Existing Summary:\n",
    "                        {existing_summary}\n",
    "\n",
    "                        New Information:\n",
    "                        {new_docs}\n",
    "\n",
    "                        Please refine the existing summary to incorporate the new information accurately and comprehensively.\n",
    "                        Helpful Answer:\"\"\"\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "refine_chain = LLMChain(llm=llm, prompt=refine_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_chain.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain2 = load_summarize_chain(llm, chain_type=\"refine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='FunctionDef main  \\nmain: The function of main is to copy Markdown documentation files from a specified folder to a destination folder, create a README.md file if it does not exist, and organize the copied files accordingly.  \\nparameters:\\n- markdown_docs_folder: The folder containing the Markdown documentation files.\\n- book_name: The name of the book being generated.\\n- repo_path: The path to the repository.  \\nCode Description:', metadata={'source': 'generate_repoagent_books.md'})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start time\n",
    "import time\n",
    "start = time.time()\n",
    "res = chain2.invoke(docs)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='FunctionDef main  \\nmain: The function of main is to copy Markdown documentation files from a specified folder to a destination folder, create a README.md file if it does not exist, and organize the copied files accordingly.  \\nparameters:\\n- markdown_docs_folder: The folder containing the Markdown documentation files.\\n- book_name: The name of the book being generated.\\n- repo_path: The path to the repository.  \\nCode Description:', metadata={'source': 'generate_repoagent_books.md'})],\n",
       " 'output_text': 'The main function copies Markdown documentation files from a specified folder to a destination folder, creates a README.md file if it does not exist, and organizes the copied files. It takes parameters for the folder containing the files, the book name, and the repository path.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
